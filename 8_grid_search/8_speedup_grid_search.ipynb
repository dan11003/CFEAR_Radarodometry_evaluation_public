{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a41dfe-ab35-4bdc-b261-6acea39d42d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/grid-search-cfear-1_2022-04-10_1134_eval.csv\n",
      "./data/grid-search-cfear-1_2022-04-10_1136_eval.csv\n",
      "WARNING WARNING, THIS PART DOES NOT WORK IN GENERAL, USE WITH CARE FOR NEW DATASETS\n",
      "./data/grid-search-cfear-2_2022-04-10_1803_eval.csv\n",
      "./data/grid-search-cfear-2_2022-04-10_1422_eval.csv\n",
      "WARNING WARNING, THIS PART DOES NOT WORK IN GENERAL, USE WITH CARE FOR NEW DATASETS\n",
      "./data/grid-search-cfear-3_2022-04-12_1057_eval.csv\n",
      "./data/grid-search-cfear-3_2022-04-14_1154_eval.csv\n",
      "WARNING WARNING, THIS PART DOES NOT WORK IN GENERAL, USE WITH CARE FOR NEW DATASETS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "pd.set_option('display.max_columns', None)\n",
    "params=['submap keyframes','k strongest','z min','resolution r [m]','Weight','loss limit','dataset','method']\n",
    "error_metrics=[\"Transl. Error[%]\", \"Rot.err.[deg/100m]\", \"RPE(m)\", \"ATE(m)\"]\n",
    "datasets=[\"oxford\",\"mulran\"]\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "def LoadData(prefix,base_dir):\n",
    "    search_string=\"/\"+prefix+\"*.csv\"\n",
    "    dfs = []\n",
    "    for filename in glob.glob(base_dir+search_string, recursive=True):\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, skipinitialspace=True)\n",
    "        dfs.append(df)\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "        df = df.rename(columns={'resolution r': 'resolution r [m]'})\n",
    "    df = df.rename(columns={'Trans.err.(%)': 'Transl. Error[%]'})\n",
    "    df = df.rename(columns={'Rot.err.(deg/100m)': 'Rot.err.[deg/100m]'})\n",
    "\n",
    "    df['for_copy'] = df.apply (lambda row: \"{:2.2f}\".format(row[\"Transl. Error[%]\"])+\"/\"+\"{:2.2f}\".format(row[\"Rot.err.[deg/100m]\"]) , axis=1)\n",
    "    df['for_copy_ATE'] = df.apply (lambda row: \"{:2.2f}\".format(row[\"RPE(m)\"])+\"/\"+\"{:2.2f}\".format(row[\"ATE(m)\"]) , axis=1)\n",
    "    df['method'].replace(['baseline-cfear-1'], 'grid-search-cfear-1') #Correct wrong namer\n",
    "    df['method'].replace(\n",
    "        to_replace=['baseline-cfear-1'],\n",
    "        value='grid-search-cfear-1',\n",
    "        inplace=True\n",
    "    )\n",
    "    print(\"WARNING WARNING, THIS PART DOES NOT WORK IN GENERAL, USE WITH CARE FOR NEW DATASETS\")\n",
    "    df = df.drop(df[df[\"Transl. Error[%]\"]<1].index)\n",
    "    \n",
    "    \n",
    "    df=df.sort_values(by=params)\n",
    "\n",
    "    return df\n",
    "    \n",
    "###### LOAD DATA ############\n",
    "#baseline='/home/daniel/rosbag/CFEAR_EVAL/eval_storage/8_grid_search_2022-04-04_2143'\n",
    "baseline='./data/'\n",
    "\n",
    "df1=LoadData(\"grid-search-cfear-1\", baseline)\n",
    "df2=LoadData(\"grid-search-cfear-2\", baseline)\n",
    "df3=LoadData(\"grid-search-cfear-3\", baseline)\n",
    "dfs=[df1,df2,df3]\n",
    "#for df in dfs:\n",
    "    #print(df)\n",
    "\n",
    "\n",
    "    #params=['submap keyframes','k strongest','z min','resolution r [m]','Weight','loss limit']\n",
    "    #full_params=params+['dataset','method']\n",
    "    #df=df.sort_values(by=full_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db758b5-5902-4ec9-a107-d7179b29579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------SCV------------\n",
      "['mulran' 'oxford']\n",
      "['grid-search-cfear-1']\n",
      "1.432\n",
      "------------SCV------------\n",
      "['mulran' 'oxford']\n",
      "['grid-search-cfear-2']\n",
      "1.227\n",
      "------------SCV------------\n",
      "['mulran' 'oxford']\n",
      "['grid-search-cfear-3']\n",
      "1.131\n"
     ]
    }
   ],
   "source": [
    "#################### NOTE, THIS SCRIPT TAKES TIME TO RUN - a few minutes ##############\n",
    "for i in range(len(dfs)):\n",
    "    \n",
    "    methods=np.array(dfs[i][\"method\"].unique())\n",
    "    datasets=np.array(dfs[i][\"dataset\"].unique())\n",
    "    print(\"------------SCV------------\")\n",
    "\n",
    "    imin=dfs[i]['Transl. Error[%]'].argmin()\n",
    "    print(datasets)\n",
    "    print(methods)\n",
    "    error=dfs[i].iloc[imin][\"Transl. Error[%]\"]\n",
    "    print(error)\n",
    "    #if(error<1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ce0ce-7e01-42cb-9cef-938fcc1101f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reducing dataframe size by averaging the error over all trajectories.\n",
    "count = 0\n",
    "columns=params+error_metrics\n",
    "table=[]\n",
    "def getParVek(row):\n",
    "    curr_pars=[]\n",
    "    for par in params:\n",
    "        curr_pars.append(row[par])\n",
    "    return curr_pars\n",
    "\n",
    "\n",
    "def CreateMinimalDataframe(dtmp):\n",
    "    dconcat=pd.DataFrame( columns=params+error_metrics)\n",
    "    \n",
    "    prev_pars=[]\n",
    "    count = 0\n",
    "    idx_vek=[]    \n",
    "    for current_index, row in dtmp.iterrows():\n",
    "        curr_pars=getParVek(row)\n",
    "        #print(curr_pars)\n",
    "        \n",
    "        if(count==0 or prev_pars==curr_pars):\n",
    "            idx_vek.append(current_index) \n",
    "        else:\n",
    "            df_curr=dtmp.loc[idx_vek]\n",
    "            #print(df_curr[\"dataset\"])\n",
    "            errors=[]\n",
    "            for error in error_metrics:\n",
    "                err=df_curr[error].mean()\n",
    "                errors.append(err)\n",
    "            idx_vek.clear()\n",
    "            idx_vek.append(current_index)            \n",
    "            dd = pd.DataFrame([[int(count)]+prev_pars+errors], columns = [\"id\"]+columns)\n",
    "            dconcat=dconcat.append(dd)\n",
    "            #print(dd)\n",
    "            \n",
    "        \n",
    "            \n",
    "        prev_pars=curr_pars\n",
    "        count = count+1\n",
    "    df_curr=dtmp.loc[idx_vek]\n",
    "    #print(idx_vek)\n",
    "    errors=[]\n",
    "    for error in error_metrics:\n",
    "        err=df_curr[error].mean()\n",
    "        errors.append(err)\n",
    "    idx_vek.clear()\n",
    "    dd = pd.DataFrame([[int(count)]+prev_pars+errors], columns = [\"id\"]+columns)\n",
    "    dconcat=dconcat.append(dd)\n",
    "    dconcat=dconcat.set_index(\"id\")\n",
    "    dconcat.reset_index() \n",
    "         \n",
    "        \n",
    "    return dconcat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296c7b07-3573-41f5-9544-488407926382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "576\n",
      "972\n"
     ]
    }
   ],
   "source": [
    "######### Reduces frames to minimal representation\n",
    "dfs_filtered=[]\n",
    "\n",
    "for di in dfs:\n",
    "    dfiltered=CreateMinimalDataframe(di)\n",
    "    dfs_filtered.append(dfiltered)\n",
    "    print(str(dfiltered.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ced5e8-d7d5-46e6-bda9-7e8dd195f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Prepare string output\n",
    "error_metrics=[\"Transl. Error[%]\", \"Rot.err.[deg/100m]\", \"RPE(m)\", \"ATE(m)\"]\n",
    "for i in range(len(dfs_filtered)):\n",
    "    dfs_filtered[i][\"mean\"] = dfs_filtered[i].apply (lambda row: \"{:2.2f}\".format(row[\"Transl. Error[%]\"])+\"/\"+\"{:2.2f}\".format(row[\"Rot.err.[deg/100m]\"]) , axis=1)         \n",
    "    dfs_filtered[i][\"Mean RPE [cm]/Mean ATE [m]\"] = dfs_filtered[i].apply (lambda row: \"{:2.2f}\".format(100*row[\"RPE(m)\"])+\"/\"+\"{:2.2f}\".format(row[\"ATE(m)\"]) , axis=1)         \n",
    "    dfs_filtered[i][\"mean SCV\"] = dfs_filtered[i].apply (lambda row: \"N/A\" , axis=1)         \n",
    "    dfs_filtered[i][\"mean Opti.\"] = dfs_filtered[i].apply (lambda row: \"N/A\" , axis=1)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37470032-6017-4ff1-b88b-45403fb44e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Evaluation dataset: mulran\n",
      "Training error: 1.63/0.57\n",
      "Evaluation error: 2.34/0.79\n",
      "-----------------------------------------\n",
      "Evaluation dataset: oxford\n",
      "Training error: 2.34/0.79\n",
      "Evaluation error: 1.69/0.60\n",
      "-----------------------------------------\n",
      "Evaluation dataset: mulran\n",
      "Training error: 1.40/0.49\n",
      "Evaluation error: 1.93/0.66\n",
      "-----------------------------------------\n",
      "Evaluation dataset: oxford\n",
      "Training error: 1.93/0.66\n",
      "Evaluation error: 1.40/0.49\n",
      "-----------------------------------------\n",
      "Evaluation dataset: mulran\n",
      "Training error: 1.20/0.39\n",
      "Evaluation error: 1.60/0.57\n",
      "-----------------------------------------\n",
      "Evaluation dataset: oxford\n",
      "Training error: 1.55/0.56\n",
      "Evaluation error: 1.22/0.39\n",
      "Evaluated dataset: mulran\n",
      "Method: grid-search-cfear-1, mean SCV and Opti. & 2.34/0.79 & 2.34/0.79\n",
      "Method: grid-search-cfear-2, mean SCV and Opti. & 1.93/0.66 & 1.93/0.66\n",
      "Method: grid-search-cfear-3, mean SCV and Opti. & 1.60/0.57 & 1.55/0.56\n",
      "Evaluated dataset: oxford\n",
      "Method: grid-search-cfear-1, mean SCV and Opti. & 1.69/0.60 & 1.63/0.57\n",
      "Method: grid-search-cfear-2, mean SCV and Opti. & 1.40/0.49 & 1.40/0.49\n",
      "Method: grid-search-cfear-3, mean SCV and Opti. & 1.22/0.39 & 1.20/0.39\n"
     ]
    }
   ],
   "source": [
    "dfs_filtered_copy = dfs_filtered.copy()\n",
    "\n",
    "for i in range(len(dfs_filtered_copy)):\n",
    "    \n",
    "    df=dfs_filtered_copy[i]\n",
    "    #print(df)\n",
    "    for dataset_eval in datasets:\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(\"Evaluation dataset: \" + dataset_eval)\n",
    "        #for method in methods:\n",
    "        for dataset_training in datasets:\n",
    "            if dataset_training != dataset_eval:\n",
    "                # GET METHOD NAME AND SUBDIVIDE PER DATASET\n",
    "                method=df[\"method\"].unique()[0]\n",
    "                df_training=df[(df[\"dataset\"]==dataset_training)] # Subset\n",
    "                df_eval=df[ (df[\"dataset\"]==dataset_eval)]\n",
    "\n",
    "                #Get training error\n",
    "                idx_argmin=df_training['Transl. Error[%]'].argmin()\n",
    "                training_argmin_frame=df_training.iloc[idx_argmin]\n",
    "                training_error_copy = training_argmin_frame[\"mean\"]\n",
    "                training_error = str(training_argmin_frame[\"Transl. Error[%]\"]) \n",
    "                   \n",
    "                #Get the corresponding frame but for the other dataset ( df_eval[\"dataset\"]!=training_argmin_frame[\"dataset\"] )\n",
    "                mask=(df_eval[\"submap keyframes\"]==training_argmin_frame[\"submap keyframes\"]) & (df_eval[\"k strongest\"]==training_argmin_frame[\"k strongest\"]) & (df_eval[\"z min\"]==training_argmin_frame[\"z min\"]) & (df_eval[\"resolution r [m]\"]==training_argmin_frame[\"resolution r [m]\"]) & (df_eval[\"Weight\"]==training_argmin_frame[\"Weight\"]) & (df_eval[\"method\"]==training_argmin_frame[\"method\"]) & (df_eval[\"loss limit\"]==training_argmin_frame[\"loss limit\"]) & (df_eval[\"dataset\"]!=training_argmin_frame[\"dataset\"])\n",
    "                eval_argmin_frame=df_eval[mask].iloc[0]\n",
    "                evaluation_error_copy = eval_argmin_frame[\"mean\"]\n",
    "                evaluation_error_drift = eval_argmin_frame[\"Transl. Error[%]\"]\n",
    "                evaluation_error_rpe=eval_argmin_frame[\"Mean RPE [cm]/Mean ATE [m]\"]\n",
    "                #df_training=df_training.set_index(range(0,df_training.shape[0]))\n",
    "                #print(df_training)\n",
    "                eval_idx_list=[]\n",
    "                train_idx_list=[]\n",
    "                for current_index, row in df_eval.iterrows():\n",
    "                    eval_idx_list.append(current_index)\n",
    "                df.loc[eval_idx_list,\"mean SCV\"] = evaluation_error_copy\n",
    "                \n",
    "                for current_index, row in df_training.iterrows():\n",
    "                    train_idx_list.append(current_index)\n",
    "                df.loc[train_idx_list,\"mean Opti.\"] = training_error_copy\n",
    "                                \n",
    "                #Optional prints\n",
    "                print(\"Training error: \" + training_error_copy)\n",
    "                #print(training_argmin_frame)\n",
    "                print(\"Evaluation error: \" + evaluation_error_copy)\n",
    "                #print(eval_argmin_frame)\n",
    "\n",
    "                #print(\"method: \"+method+\", \"+dataset_training+\", Mean Opti. =\"+training_error_copy)\n",
    "                #print(\"method: \"+method+\", ,\"+dataset_eval+\", Mean SCV. =\"+evaluation_error_copy)\n",
    "                #print(\"method: \"+method+\", ,\"+dataset_eval+\", RPE/ATE SCV. =\"+evaluation_error_rpe)\n",
    "                df.to_csv('./output/'+method+'.csv', index=False)  \n",
    "        \n",
    "\n",
    "for dataset_eval in datasets:\n",
    "    print(\"Evaluated dataset: \" + dataset_eval)\n",
    "    for df in dfs_filtered_copy:\n",
    "        drow=df[df[\"dataset\"]==dataset_eval].iloc[0]\n",
    "        #print(drow)\n",
    "        print( \"Method: \"+drow[\"method\"]+\", mean SCV and Opti. & \" + drow[\"mean SCV\"] + \" & \" + drow[\"mean Opti.\"])\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
